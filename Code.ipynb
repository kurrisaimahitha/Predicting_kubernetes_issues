{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                             roc_curve, precision_recall_curve)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class KubernetesFailurePrediction:\n",
        "    def __init__(self, data_paths, log_path=\"prediction_logs.txt\"):\n",
        "        \"\"\"\n",
        "        Initialize the Kubernetes Failure Prediction model.\n",
        "\n",
        "        Parameters:\n",
        "        data_paths (dict): Dictionary containing paths to the data files\n",
        "        log_path (str): Path to save logs\n",
        "        \"\"\"\n",
        "        self.data_paths = data_paths\n",
        "        self.log_path = log_path\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.feature_names = None\n",
        "        self.feature_importances = None\n",
        "        self.label_mapping = {\"benign\": 0, \"normal\": 0, \"operational\": 0,\n",
        "                             \"malicious\": 1, \"failure\": 1, \"disruption\": 1,\n",
        "                             \"resource_exhaustion\": 2, \"network_issue\": 3}\n",
        "\n",
        "        # Open log file\n",
        "        self.log_file = open(self.log_path, 'w')\n",
        "        self.log(\"Kubernetes Failure Prediction Model initialized\")\n",
        "\n",
        "    def log(self, message):\n",
        "        \"\"\"Log a message to both console and log file\"\"\"\n",
        "        print(message)\n",
        "        self.log_file.write(message + \"\\n\")\n",
        "        self.log_file.flush()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and combine datasets from provided paths\"\"\"\n",
        "        self.log(\"Loading datasets...\")\n",
        "\n",
        "        dataframes = []\n",
        "        for name, path in self.data_paths.items():\n",
        "            try:\n",
        "                df = pd.read_csv(path, on_bad_lines='skip', low_memory=False)\n",
        "                self.log(f\"Loaded {name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "                dataframes.append(df)\n",
        "            except Exception as e:\n",
        "                self.log(f\"Error loading {name}: {str(e)}\")\n",
        "\n",
        "        # Combine all dataframes\n",
        "        self.df_combined = pd.concat(dataframes, ignore_index=True)\n",
        "        self.log(f\"Combined dataset shape: {self.df_combined.shape}\")\n",
        "\n",
        "        return self.df_combined\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Clean and preprocess the combined dataset\"\"\"\n",
        "        self.log(\"Preprocessing data...\")\n",
        "\n",
        "        df = self.df_combined\n",
        "\n",
        "        # Clean up column names\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        cols_to_drop = [\"_source_flow_id\", \"_source_flow_final\", \"_source_source_ip\", \"_source_destination_ip\"]\n",
        "        df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
        "\n",
        "        # Handle labels - map to Kubernetes failure types\n",
        "        if \"label\" in df.columns:\n",
        "            df.rename(columns={\"label\": \"failure_label\"}, inplace=True)\n",
        "\n",
        "        # Map labels to Kubernetes failure categories\n",
        "        if \"failure_label\" in df.columns:\n",
        "            df[\"failure_label\"] = df[\"failure_label\"].map(lambda x: self.label_mapping.get(str(x).lower(), 0))\n",
        "        else:\n",
        "            self.log(\"Warning: 'failure_label' column not found. Creating synthetic labels for demonstration.\")\n",
        "            df[\"failure_label\"] = 0  # Default to normal\n",
        "\n",
        "            # Create synthetic labels based on network patterns\n",
        "            if \"_source_network_bytes\" in df.columns:\n",
        "                network_threshold = df[\"_source_network_bytes\"].quantile(0.95)\n",
        "                df.loc[df[\"_source_network_bytes\"] > network_threshold, \"failure_label\"] = 3  # Network issue\n",
        "\n",
        "            if \"_source_event_duration\" in df.columns:\n",
        "                duration_threshold = df[\"_source_event_duration\"].quantile(0.9)\n",
        "                df.loc[df[\"_source_event_duration\"] > duration_threshold, \"failure_label\"] = 2  # Resource exhaustion\n",
        "\n",
        "            if \"_source_network_transport\" in df.columns:\n",
        "                df.loc[df[\"_source_network_transport\"].astype(str) == \"icmp\", \"failure_label\"] = 1  # Pod failure\n",
        "\n",
        "        # Ensure failure_label is numeric\n",
        "        df[\"failure_label\"] = pd.to_numeric(df[\"failure_label\"], errors=\"coerce\")\n",
        "\n",
        "        # Handle missing values\n",
        "        df.fillna(0, inplace=True)\n",
        "\n",
        "        # Convert categorical features\n",
        "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "        encoder = LabelEncoder()\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            df[col] = df[col].astype(str)\n",
        "            df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "        # Convert numeric features\n",
        "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "        for col in numeric_cols:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "            df[col].fillna(0, inplace=True)\n",
        "\n",
        "        # Create new features for Kubernetes-specific metrics\n",
        "        if \"_source_event_duration\" in df.columns:\n",
        "            df[\"simulated_cpu_usage\"] = df[\"_source_event_duration\"] / df[\"_source_event_duration\"].max() * 100\n",
        "\n",
        "        if \"_source_network_bytes\" in df.columns:\n",
        "            df[\"simulated_memory_usage\"] = df[\"_source_network_bytes\"] / df[\"_source_network_bytes\"].max() * 100\n",
        "\n",
        "        # Simulate disk usage\n",
        "        df[\"simulated_disk_usage\"] = np.random.uniform(10, 95, size=df.shape[0])\n",
        "\n",
        "        # Simulate pod restart count\n",
        "        df[\"simulated_pod_restarts\"] = np.random.randint(0, 5, size=df.shape[0])\n",
        "        df.loc[df[\"failure_label\"] == 1, \"simulated_pod_restarts\"] = np.random.randint(2, 10,\n",
        "                                                                                       size=df[df[\"failure_label\"] == 1].shape[0])\n",
        "\n",
        "        self.log(f\"Data preprocessing completed. Final shape: {df.shape}\")\n",
        "        self.preprocessed_data = df\n",
        "        return df\n",
        "\n",
        "    def select_features(self):\n",
        "        \"\"\"Select relevant features for the model\"\"\"\n",
        "        self.log(\"Selecting features...\")\n",
        "\n",
        "        df = self.preprocessed_data\n",
        "\n",
        "        # Define base features\n",
        "        base_features = [\n",
        "            \"_source_network_bytes\",\n",
        "            \"_source_event_duration\",\n",
        "            \"_source_network_transport\"\n",
        "        ]\n",
        "\n",
        "        # Add simulated Kubernetes metrics\n",
        "        k8s_features = [\n",
        "            \"simulated_cpu_usage\",\n",
        "            \"simulated_memory_usage\",\n",
        "            \"simulated_pod_restarts\"  # Removed 'simulated_disk_usage' as it has low importance\n",
        "        ]\n",
        "\n",
        "        # Combine all features\n",
        "        all_features = []\n",
        "        for feature in base_features + k8s_features:\n",
        "            if feature in df.columns:\n",
        "                all_features.append(feature)\n",
        "\n",
        "        self.log(f\"Selected features: {all_features}\")\n",
        "        self.feature_names = all_features\n",
        "\n",
        "        # Create feature matrix and target vector\n",
        "        X = df[all_features]\n",
        "        y = df[\"failure_label\"]\n",
        "\n",
        "        # Scale features\n",
        "        self.scaler = StandardScaler()\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        self.log(f\"Feature matrix shape: {X.shape}, Target vector shape: {y.shape}\")\n",
        "\n",
        "        return X_scaled, y\n",
        "\n",
        "    def train_test_chronological_split(self, X, y, test_size=0.2):\n",
        "        \"\"\"Split data chronologically for time series data\"\"\"\n",
        "        self.log(f\"Splitting data chronologically with test_size={test_size}\")\n",
        "\n",
        "        # For time series data, we split chronologically\n",
        "        split_idx = int(len(X) * (1 - test_size))\n",
        "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        self.log(f\"Training set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def handle_imbalanced_data(self, X_train, y_train):\n",
        "        \"\"\"Apply SMOTE to handle class imbalance\"\"\"\n",
        "        self.log(\"Applying SMOTE to handle class imbalance...\")\n",
        "\n",
        "        # Get class distribution before SMOTE\n",
        "        class_dist_before = pd.Series(y_train).value_counts(normalize=True)\n",
        "        self.log(f\"Class distribution before SMOTE: {class_dist_before.to_dict()}\")\n",
        "\n",
        "        # Apply SMOTE\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Get class distribution after SMOTE\n",
        "        class_dist_after = pd.Series(y_train_balanced).value_counts(normalize=True)\n",
        "        self.log(f\"Class distribution after SMOTE: {class_dist_after.to_dict()}\")\n",
        "\n",
        "        return X_train_balanced, y_train_balanced\n",
        "\n",
        "    def train_model(self, X_train, y_train):\n",
        "        \"\"\"Train the model with XGBoost\"\"\"\n",
        "        self.log(\"Training XGBoost model...\")\n",
        "\n",
        "        # Initialize and train the model\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=10,  # Adjust for class imbalance\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        self.model = model\n",
        "        self.log(\"Model training completed\")\n",
        "\n",
        "        # Extract feature importances\n",
        "        self.feature_importances = pd.DataFrame({\n",
        "            'Feature': self.feature_names,\n",
        "            'Importance': model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        self.log(\"\\nFeature Importance:\")\n",
        "        self.log(self.feature_importances.to_string())\n",
        "\n",
        "        return model\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test, threshold=0.3):\n",
        "        \"\"\"Evaluate the trained model on test data\"\"\"\n",
        "        self.log(\"Evaluating model...\")\n",
        "\n",
        "        # Make predictions with adjusted threshold\n",
        "        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
        "        y_pred = (y_pred_proba > threshold).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        # Print metrics\n",
        "        self.log(f\"\\nModel Evaluation Metrics:\")\n",
        "        self.log(f\"Accuracy: {accuracy:.4f}\")\n",
        "        self.log(f\"Precision: {precision:.4f}\")\n",
        "        self.log(f\"Recall: {recall:.4f}\")\n",
        "        self.log(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "        # Print classification report\n",
        "        class_report = classification_report(y_test, y_pred)\n",
        "        self.log(\"\\nClassification Report:\")\n",
        "        self.log(class_report)\n",
        "\n",
        "        # Print confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        self.log(\"\\nConfusion Matrix:\")\n",
        "        self.log(str(cm))\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        self.plot_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        # Plot ROC curve\n",
        "        self.plot_roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "        # Return evaluation metrics as a dictionary\n",
        "        eval_metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'classification_report': class_report,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "        return eval_metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, y_test, y_pred, output_path=\"confusion_matrix.png\"):\n",
        "        \"\"\"Plot confusion matrix and save to file\"\"\"\n",
        "        self.log(\"Plotting confusion matrix...\")\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Normal', 'Pod Failure', 'Resource Exhaustion', 'Network Issue'],\n",
        "                   yticklabels=['Normal', 'Pod Failure', 'Resource Exhaustion', 'Network Issue'])\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Confusion Matrix for Kubernetes Failure Prediction')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_path)\n",
        "        plt.close()\n",
        "\n",
        "        self.log(f\"Confusion matrix plot saved to {output_path}\")\n",
        "\n",
        "    def plot_roc_curve(self, y_test, y_pred_proba, output_path=\"roc_curve.png\"):\n",
        "        \"\"\"Plot ROC curve for binary classification and save to file\"\"\"\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            self.log(\"Plotting ROC curve...\")\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                    label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title('ROC Curve for Kubernetes Failure Prediction')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.savefig(output_path)\n",
        "            plt.close()\n",
        "\n",
        "            self.log(f\"ROC curve plot saved to {output_path}\")\n",
        "\n",
        "    def save_model(self, model_path=\"kubernetes_failure_prediction_model.joblib\"):\n",
        "        \"\"\"Save the trained model to disk\"\"\"\n",
        "        self.log(f\"Saving model to {model_path}...\")\n",
        "\n",
        "        model_data = {\n",
        "            'model': self.model,\n",
        "            'scaler': self.scaler,\n",
        "            'feature_names': self.feature_names,\n",
        "            'feature_importances': self.feature_importances,\n",
        "            'label_mapping': self.label_mapping\n",
        "        }\n",
        "\n",
        "        joblib.dump(model_data, model_path)\n",
        "        self.log(\"Model saved successfully\")\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the log file\"\"\"\n",
        "        if hasattr(self, 'log_file') and self.log_file:\n",
        "            self.log(\"Closing log file\")\n",
        "            self.log_file.close()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths to data files\n",
        "    data_paths = {\n",
        "        \"feb2022\": \"elastic_february2022_data.csv\",\n",
        "        \"may2021_benign\": \"elastic_may2021_benign_data.csv\",\n",
        "        \"may2021_malicious\": \"elastic_may2021_malicious_data.csv\",\n",
        "        \"may2022\": \"elastic_may2022_data.csv\"\n",
        "    }\n",
        "\n",
        "    # Initialize the predictor\n",
        "    predictor = KubernetesFailurePrediction(data_paths)\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    try:\n",
        "        # Load and preprocess data\n",
        "        predictor.load_data()\n",
        "        predictor.preprocess_data()\n",
        "\n",
        "        # Select features\n",
        "        X, y = predictor.select_features()\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = predictor.train_test_chronological_split(X, y)\n",
        "\n",
        "        # Handle imbalanced data\n",
        "        X_train_balanced, y_train_balanced = predictor.handle_imbalanced_data(X_train, y_train)\n",
        "\n",
        "        # Train model\n",
        "        predictor.train_model(X_train_balanced, y_train_balanced)\n",
        "\n",
        "        # Evaluate model with adjusted threshold\n",
        "        eval_metrics = predictor.evaluate_model(X_test, y_test, threshold=0.3)\n",
        "\n",
        "        # Save model\n",
        "        predictor.save_model()\n",
        "\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "        print(f\"Model accuracy: {eval_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Model F1-score: {eval_metrics['f1']:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during pipeline execution: {str(e)}\")\n",
        "    finally:\n",
        "        predictor.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBAHZVTr_yGo",
        "outputId": "e257c2b8-982d-48e4-8623-95c6c6063a49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kubernetes Failure Prediction Model initialized\n",
            "Loading datasets...\n",
            "Loaded feb2022: 398414 rows, 11 columns\n",
            "Loaded may2021_benign: 1777238 rows, 11 columns\n",
            "Loaded may2021_malicious: 1954212 rows, 11 columns\n",
            "Loaded may2022: 961046 rows, 1 columns\n",
            "Combined dataset shape: (5090910, 12)\n",
            "Preprocessing data...\n",
            "Data preprocessing completed. Final shape: (5090910, 12)\n",
            "Selecting features...\n",
            "Selected features: ['_source_network_bytes', '_source_event_duration', '_source_network_transport', 'simulated_cpu_usage', 'simulated_memory_usage', 'simulated_pod_restarts']\n",
            "Feature matrix shape: (5090910, 6), Target vector shape: (5090910,)\n",
            "Splitting data chronologically with test_size=0.2\n",
            "Training set: 4072728 samples, Test set: 1018182 samples\n",
            "Applying SMOTE to handle class imbalance...\n",
            "Class distribution before SMOTE: {0: 0.9485966163220328, 1: 0.0514033836779672}\n",
            "Class distribution after SMOTE: {0: 0.5, 1: 0.5}\n",
            "Training XGBoost model...\n",
            "Model training completed\n",
            "\n",
            "Feature Importance:\n",
            "                     Feature  Importance\n",
            "5     simulated_pod_restarts    0.554633\n",
            "0      _source_network_bytes    0.372712\n",
            "1     _source_event_duration    0.057545\n",
            "2  _source_network_transport    0.015110\n",
            "3        simulated_cpu_usage    0.000000\n",
            "4     simulated_memory_usage    0.000000\n",
            "Evaluating model...\n",
            "\n",
            "Model Evaluation Metrics:\n",
            "Accuracy: 0.9958\n",
            "Precision: 0.9994\n",
            "Recall: 0.9958\n",
            "F1-Score: 0.9975\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00   1017626\n",
            "           1       0.10      0.86      0.18       556\n",
            "\n",
            "    accuracy                           1.00   1018182\n",
            "   macro avg       0.55      0.93      0.59   1018182\n",
            "weighted avg       1.00      1.00      1.00   1018182\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1013449    4177]\n",
            " [     78     478]]\n",
            "Plotting confusion matrix...\n",
            "Confusion matrix plot saved to confusion_matrix.png\n",
            "Plotting ROC curve...\n",
            "ROC curve plot saved to roc_curve.png\n",
            "Saving model to kubernetes_failure_prediction_model.joblib...\n",
            "Model saved successfully\n",
            "\n",
            "Pipeline completed successfully!\n",
            "Model accuracy: 0.9958\n",
            "Model F1-score: 0.9975\n",
            "Closing log file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Reverse label mapping for better readability\n",
        "reverse_label_mapping = {v: k for k, v in {\n",
        "    \"benign\": 0, \"normal\": 0, \"operational\": 0,\n",
        "    \"malicious\": 1, \"failure\": 1, \"disruption\": 1,\n",
        "    \"resource_exhaustion\": 2, \"network_issue\": 3\n",
        "}.items()}\n",
        "\n",
        "def stratified_sample_predictions(predictor, X_test, y_test, samples_per_class=3):\n",
        "    \"\"\"Ensure that test samples cover all label categories.\"\"\"\n",
        "\n",
        "    # Convert test data to DataFrame\n",
        "    test_df = pd.DataFrame(X_test, columns=predictor.feature_names)\n",
        "    test_df[\"actual_label\"] = y_test.values\n",
        "\n",
        "    # Create a new column with actual category names\n",
        "    test_df[\"actual_category\"] = test_df[\"actual_label\"].map(reverse_label_mapping)\n",
        "\n",
        "    # Collect samples from each class\n",
        "    sampled_data = test_df.groupby(\"actual_label\").apply(\n",
        "        lambda x: x.sample(min(samples_per_class, len(x)), random_state=42)\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # Get predictions\n",
        "    sampled_features = sampled_data[predictor.feature_names]\n",
        "    sampled_preds = predictor.model.predict(sampled_features)  # Predict class labels\n",
        "\n",
        "    # Add predictions to the DataFrame\n",
        "    sampled_data[\"predicted_label\"] = sampled_preds\n",
        "    sampled_data[\"predicted_category\"] = sampled_data[\"predicted_label\"].map(reverse_label_mapping)\n",
        "\n",
        "    print(\"\\nStratified Sampled Predictions (Actual vs Predicted):\")\n",
        "    print(sampled_data[[\"actual_category\", \"predicted_category\"]])\n",
        "\n",
        "    return sampled_data\n",
        "\n",
        "# Run sampling function with at least 3 samples per class\n",
        "sampled_results = stratified_sample_predictions(predictor, X_test, y_test, samples_per_class=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnAya-jAdinO",
        "outputId": "645aaefa-fe6c-4f00-a555-998460b12aa0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stratified Sampled Predictions (Actual vs Predicted):\n",
            "  actual_category predicted_category\n",
            "0     operational        operational\n",
            "1     operational        operational\n",
            "2     operational        operational\n",
            "3      disruption         disruption\n",
            "4      disruption         disruption\n",
            "5      disruption         disruption\n"
          ]
        }
      ]
    }
  ]
}